\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may
                            % need BEFORE the SCITEPRESS.sty package.
\usepackage{hyperref}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}
<<setup, cache=FALSE,echo=FALSE>>=
library("ggplot2")
library("RCurl") #To download stuff directly from the GitHub repo
# Now download stuff from noisy-ga repo
made.data <- getURL("https://github.com/JJ/noisy-ga/raw/master/data/MADE/made-data.csv")
pacman.data <- getURL("https://github.com/JJ/noisy-ga/raw/master/data/ms-pacman/pacman.Rdata")
planetwars.data <- getURL("https://github.com/JJ/noisy-ga/raw/master/data/planet-wars/planetwars.Rdata")
@ 

\title{There is Noisy Lunch: a study of noise in evolutionary
  optimization problems.}

\author{\authorname{Juan J. Merelo\sup{1}, Federico Liberatore,
    Antonio Fernández Ares, Rubén García\sup{2}, Zeineb Chelly, Carlos
    Cotta and Nuria Rico\sup{1}}
\affiliation{\sup{1}CITIC and University of Granada}
\affiliation{\sup{2}Escuela de Doctorado, UGR}
\email{{\tt jmerelo@geneura.ugr.es}}
}

\keywords{games, evolutionary optimization, noise, uncertainty}

\abstract{Noise or uncertainty appear in many optimization processes
  when there is not a single measure of optimality or fitness but a
  random variable representing it. This
  has been known for a long time, but there has been no investigation
  of the statistical distribution that random variable follow,
  assuming in most cases that it is distributed normally and, thus,
  can be modelled via an additive or multiplicative noise. In this
  paper we will look at several uncertain optimization problems and
  prove that there is no single statistical model these variables
  follow, with different not only from one problem to the next, but in
different phases of the optimization in a single problem.}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

Optimization methods usually need a crisp and fixed value to work
correctly. This value, usually called {\em cost} or {\em fitness},
informs the procedure on how good is the solution and is used to
select particular solution over others. In fact, most procedures do
not need a floating point number; since they are based on comparisons,
it is usually enough that the values can be partially ordered;
multiobjective optimization, in general, just need to know when
comparing two solutions whether one or the other is the best or there
can be no comparison between them. In either case, the answer to the
cuestion ``Is this solution better than the other'' is either a crisp
Yes or No of simply ``Impossible to know''.

In many cases, however, there is not a single value that describes the
performance of a solution. In case where there is uncertainty in the
measure, that is, in most physical cases, or in the procedure used to
evaluate the solution, for instance, when using a stochastic procedure
to measure it, the best way to describe a solution will be a random
variable, not a single, or even a vector, value. In our research we
have found this happens in many different problems:\begin{itemize}
\item When optimizing the layout of a webpage using simulated
  annealing \cite{jj-ppsn98}. Since simulated annealing is a
  stochastic procedure, the fitness obtained by a solution will be
 a random variable.
\item Optimizing any kind of neural network, such as
  \cite{esann94,merelo:ESNN}. Since training a neural network is a
  stochastic procedure, the error rate obtained after every training
  run will follow also a statistical distribution.
\item Evolution of game bots \cite{bots:evostar}. In this case, the
  uncertainty arises from the problem itself; in games, initial
  positions or oponent behavior have a certain stochastic component so
  that final score will also be {\em uncertain} or {\em noisy}
\end{itemize}

After some initial study of noise in \cite{merelo14:noisy}, where our
findings indicated that, in some cases, noise followed a Gamma, that
is a skewed normal distribution and proposing a solution to this using
Wilcoxon comparison as a selection operator, we dug into data
discovering that, even if the distribution in that particular case was
always a gamma, the parameters of the distribution were different,
which meant that the random variable behaved in different ways
depending on the particular individual, the state of evolution and, of
course, the particular problem.

This initial conclusion disagrees with the usual assumptions in
optimization in uncertain environments, where it is usual to assume
that the noise is normally distributed and with a fixed sigma
\cite{arnold2001evolution}. For instance, in the Black Box
Optimization Benchmarks \cite{hansen2009real} the uncertainty was
simulated by adding noise centered in 0 and with a Cauchy distribution
with different widths. Either multiplicative or additive noise has
been used in different occasions. 

That is why in this paper we have collected data from three different
problems and tried to find a model for the fitness using statistical
tools. Our intention is to eventually find a model that is as general
as possible and that is able to account for most sources of
uncertainty; failing that, to try and find selection operators that
are able to work with random variables in a natural way. However, this
is not the focus of this paper and, if it is eventually needed, is
left as future work. 

The rest of the paper is organized as follows. Next we present the
state of the art in evolutionary algorithms in uncertain environments,
to be followed by a short presentation of the three problems with
uncertainty whose measures will be used in this paper in Section
\ref{sec:problems}. Results will be presented in Section \ref{sec:res}
followed by our conclusions.

\section{\uppercase{State of the Art}}
\label{sec:soa}

\sloppypar The most recent and comprehensive review of the state of the art in evolutionary
algorithms in {\em uncertain} environments 
was done by 
\cite{Jin2005303}, although recent
papers such as \cite{DBLP:journals/corr/QianYZ13,6931307} and
\cite{Qian:sampling} include a brief updates. In that first survey  
of evolutionary optimization in
uncertain environments the authors state that uncertainty is categorized into noise,
robustness issues, fitness approximation and time-varying fitness functions, and then,
different options for dealing with it are proposed. In principle, the
approach presented in this paper was designed to deal with the first kind of
uncertainty, noise or uncertainty in fitness evaluation, although it could be argued
that there is uncertainty in the true fitness as stated in the third
category; however, we do not think that is the case and in general
that third case refers to the case in which expensive fitness
functions are substituted by surrogate functions which carry a certain
amount of error. They suggest several methods, based either on using
averaging or using a selection threshold over which one or other
individual is selected. But since then, several other solutions have
been proposed.

\sloppypar For scientists not concerned on solving the problem of noise, but on
a straightforward solution of the optimization problem without
modification of existing tools and methodologies, a usual approach is
just to disregard the fact that the fitness is 
noisy and use whatever value is returned by a single evaluation or after
re-evaluation each generation. This was the option in our
previous research in games \cite{bots:evostar,DBLP:journals/jcst/MoraFGGF12,liberatore:pacman} and evolution of neural networks \cite{castilloGECCO99,merelo:ESNN} and leads, if
the population is large enough, to an {\em implicit averaging} as
mentioned in \cite{Jin2005303}. In fact, selection used in evolutionary algorithms
is also stochastic, so noise in fitness evaluation
will have the same effect as randomness in selection or a higher mutation
rate, which might make the evolution process easier and not harder
in some particular cases
\cite{DBLP:journals/corr/QianYZ13}. 
In fact, Miller and Goldberg proved that an infinite population would not
be affected by noise \cite{miller1996genetic} and Jun-Hua and Ming studied the
effect of noise in convergence rates \cite{Junhua20136780}, proving
that an elitist genetic algorithm finds at least one solution, although with a lowered
convergence rate. But real populations are finite, so the usual
approach to dealing with fitness with a degree of randomness is to
increase the population size to a value bigger than 
would be needed in a non-noisy environment. In fact, it has been
recently proved that using {\em sex}, that is, crossover, is able to
deal successfully with noise \cite{2015arXiv150202793F}, while an
evolutionary algorithm based on mutation
%CARLOS: un algorithm mu+1 puede usar cruce - elimino esto  (such as
%the $\mu$+1 EA)
% but that's the one mentioned in the paper - JJ
would suffer a considerable degradation of performance. 
% Zeineb - (such as the $\mu$+1 EA) would suffer from a considerable degradation in terms of performance.
However, crossover is part of the standard kit of evolutionary
algorithms, so using it and increasing the population size has the
advantage that no special provision or change to the implementation
has to be made, just different values of the standard parameters.

Another more theoretically sound way is using a statistical central tendency
indicator, which is usually the {\em average}. This strategy is called
{\em explicit averaging} by Jin and Branke and is used, for instance,
in 
\cite{Junhua20136780}. Averaging decreases the variance of fitness but
the problem is that it is not clear in advance what would be the
sample size used for averaging \cite{aizawa1994scheduling}. Most
authors use several measures of fitness for each new individual
\cite{costa2013using}, although other averaging strategies have also
been proposed, like averaging over the neighbourhood of the
individual or using {\em resampling}, that is, more measures of fitness in a
number which is decided heuristically
\cite{liu2014mathematically}. This assumes that there is, effectively,
an average of the 
fitness values which is true for Gaussian random noise and other
distributions such as Gamma or Cauchy but not
necessarily for all distributions. 
To the best of our knowledge, 
other measures like the median which might be more adequate for
certain noise models have not been tested; the median always exists,
while the average might not exist for non-centrally distributed
variables. Besides, most models keep the number of evaluations fixed 
and independent of its value, 
% Zeineb - Besides, most models keep the number of evaluations   fixed
% and independent of its value, 
% fixed, thx- JJ
which might result in bad individuals
being evaluated many times before being discarded; some authors have
proposed {\em resampling}, that is, re-evaluate the individuals a number of times to increase the precision in fitness
\cite{RadaVilela2014,6900521}, 
which will effectively increase the number of
evaluations and thus slow down the search. In any case, using average is
also a small change to the overall algorithm framework, requiring only
using as new fitness function the average of several evaluations.
We will try to address this in the model presented in this
paper. 

These two approaches that are focused on the evaluation process might
be complemented with changes to the selection process. For instance,
using a threshold \cite{Rudolph2001318,6900521} that is related to the noise characteristics to
avoid making comparisons of individuals that might, in fact, be very
similar or statistically the same; this is usually called {\em
  threshold selection} and can be applied either to explicit or
implicit averaging fitness functions. The algorithms used for
solution, themselves, can be also tested, with some authors proposing, instead of taking more measures, 
testing different solvers \cite{cauwet2014algorithm}, some of which
might be more affected by noise than others. However, recent papers
have proved that sampling might be ineffective \cite{Qian:sampling} in
some types of evolutionary algorithms, adding running time without an
additional benefit in terms of performance. This is one lead we will
use in the current paper. 

Any of these approaches do have the problem of statistical
representation of the {\em true} fitness, even more so if there is not
such a thing, but several measures that represent, {\em as a set}
the fitness of an individual. This is what we are going to use in this
paper, where we present a method that uses resampling via an
individual memory and use either explicit averaging or statistical
tests like the non-parametric Wilcoxon test. 
First we will examine and try to find the shape of the noise that
actually appears in games; then we will check in this paper what is the influence on the quality of
results of these two strategies and which one, if any, is the best
when working in noisy environments.  


\section{\uppercase{Conclusions}}
\label{sec:conclusion}

\noindent Please note that ONLY the files required to compile your paper should be submitted. Previous versions or examples MUST be removed from the compilation directory before submission.

We hope you find the information in this template useful in the preparation of your submission.

\section*{\uppercase{Acknowledgements}}

\noindent This work has been supported in part by SIPESCA (Programa Operativo
FEDER de Andalucía 2007-2013), TIN2014-56494-C4-3-P (Spanish
Ministry of Economy and Competitivity), SPIP2014-01437 (Direcci{\'o}n
General de Tr{\'a}fico), PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza
Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de
Investigaci{\'o}n), and PYR-2014-17 GENIL project (CEI-BIOTIC
Granada).

\vfill

\bibliographystyle{apalike}
\bibliography{geneura,GA-general,noisy}

\end{document}

