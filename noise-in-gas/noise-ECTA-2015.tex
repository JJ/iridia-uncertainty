\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may
                            % need BEFORE the SCITEPRESS.sty package.
\usepackage{hyperref}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{There is Noisy Lunch: a study of noise in evolutionary
  optimization problems. }

\author{\authorname{Juan J. Merelo\sup{1}, Federico Liberatore,
    Antonio Fernández Ares, Rubén García\sup{2}, Zeineb Chelly, Carlos
    Cotta and Nuria Rico\sup{1}}
\affiliation{\sup{1}CITIC and University of Granada}
\affiliation{\sup{2}Afiliación}
\email{{\tt jmerelo@geneura.ugr.es}}
}

\keywords{games, evolutionary optimization, noise, uncertainty}

\abstract{}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

\section{\uppercase{State of the Art}}
\label{sec:soa}

\sloppypar The most comprehensive review of the state of the art in evolutionary
algorithms in {\em uncertain} environments 
was done by %Jin and Branke in 2005 -- CARLOS: inline reference; no need to add authors in text
\cite{Jin2005303}, although recent
papers such as \cite{DBLP:journals/corr/QianYZ13,6931307} and \cite{Qian:sampling} include a brief update of the state
of the art. In that first survey 
of evolutionary optimization by Jin and Branke  in
uncertain environments that uncertainty is categorized into noise,
robustness, fitness approximation and time-varying fitness functions, and then,
different options for dealing with it are proposed. In principle, the
approach presented in this paper was designed to deal with the first kind of
uncertainty, noise in fitness evaluation, although it could be argued
that there is uncertainty in the true fitness as stated in the third
category. In any case it could be applied to
other types of noise, since it depends more on the shape and intensity
of noise than the origin. 

\sloppypar In the same situation, a noisy fitness evaluation, several solutions have been
proposed and explained in the survey \cite{Jin2005303}. These will be
explained next.
For scientists not concerned on solving the problem of noise, but on
a straightforward solution of the optimization problem without
modification of existing tools and methodologies, a usual approach is
just to disregard the fact that the fitness is 
noisy and use whatever value is returned by a single evaluation or after
re-evaluation each generation. 
% Zeineb - A usual approach is just to disregard the fact that the fitness is noisy and 
%to use whatever value is returned a single time or after a
%re-evaluation of each generation.
% fixed, thx - JJ
This was the option in our
%CARLOS: references split in order to avoid margin problems
previous research in games \cite{bots:evostar,DBLP:journals/jcst/MoraFGGF12,Liberatore_COSECIVI14} and evolution of neural networks \cite{castilloGECCO99,merelo:ESNN} and leads, if
the population is large enough, to an {\em implicit averaging} as
mentioned in \cite{Jin2005303}. In fact, selection used in evolutionary algorithms
is also stochastic, so noise in fitness evaluation
will have the same effect as randomness in selection or a higher mutation
rate, which might make the evolution process easier and not harder
in some particular cases
\cite{DBLP:journals/corr/QianYZ13}. 
In fact, Miller and Goldberg proved that an infinite population would not
be affected by noise \cite{miller1996genetic} and Jun-Hua and Ming studied the
effect of noise in convergence rates \cite{Junhua20136780}, proving
that an elitist genetic algorithm finds at least one solution, although with a lowered
convergence rate. But real populations are finite, so the usual
approach to dealing with fitness with a degree of randomness is to
increase the population size to a value bigger than 
would be needed in a non-noisy environment. In fact, it has been
recently proved that using {\em sex}, that is, crossover, is able to
deal successfully with noise \cite{2015arXiv150202793F}, while an
evolutionary algorithm based on mutation
%CARLOS: un algorithm mu+1 puede usar cruce - elimino esto  (such as
%the $\mu$+1 EA)
% but that's the one mentioned in the paper - JJ
would suffer a considerable degradation of performance. 
% Zeineb - (such as the $\mu$+1 EA) would suffer from a considerable degradation in terms of performance.
However, crossover is part of the standard kit of evolutionary
algorithms, so using it and increasing the population size has the
advantage that no special provision or change to the implementation
has to be made, just different values of the standard parameters.

Another more theoretically sound way is using a statistical central tendency
indicator, which is usually the {\em average}. This strategy is called
{\em explicit averaging} by Jin and Branke and is used, for instance,
in 
\cite{Junhua20136780}. Averaging decreases the variance of fitness but
the problem is that it is not clear in advance what would be the
sample size used for averaging \cite{aizawa1994scheduling}. Most
authors use several measures of fitness for each new individual
\cite{costa2013using}, although other averaging strategies have also
been proposed, like averaging over the neighbourhood of the
individual or using {\em resampling}, that is, more measures of fitness in a
number which is decided heuristically
\cite{liu2014mathematically}. This assumes that there is, effectively,
an average of the 
fitness values which is true for Gaussian random noise and other
distributions such as Gamma or Cauchy but not
necessarily for all distributions. 
To the best of our knowledge, 
other measures like the median which might be more adequate for
certain noise models have not been tested; the median always exists,
while the average might not exist for non-centrally distributed
variables. Besides, most models keep the number of evaluations fixed 
and independent of its value, 
% Zeineb - Besides, most models keep the number of evaluations   fixed
% and independent of its value, 
% fixed, thx- JJ
which might result in bad individuals
being evaluated many times before being discarded; some authors have
proposed {\em resampling}, that is, re-evaluate the individuals a number of times to increase the precision in fitness
\cite{RadaVilela2014,6900521}, 
which will effectively increase the number of
evaluations and thus slow down the search. In any case, using average is
also a small change to the overall algorithm framework, requiring only
using as new fitness function the average of several evaluations.
We will try to address this in the model presented in this
paper. 

These two approaches that are focused on the evaluation process might
be complemented with changes to the selection process. For instance,
using a threshold \cite{Rudolph2001318,6900521} that is related to the noise characteristics to
avoid making comparisons of individuals that might, in fact, be very
similar or statistically the same; this is usually called {\em
  threshold selection} and can be applied either to explicit or
implicit averaging fitness functions. The algorithms used for
solution, themselves, can be also tested, with some authors proposing, instead of taking more measures, 
testing different solvers \cite{cauwet2014algorithm}, some of which
might be more affected by noise than others. However, recent papers
have proved that sampling might be ineffective \cite{Qian:sampling} in
some types of evolutionary algorithms, adding running time without an
additional benefit in terms of performance. This is one lead we will
use in the current paper. 

Any of these approaches do have the problem of statistical
representation of the {\em true} fitness, even more so if there is not
such a thing, but several measures that represent, {\em as a set}
the fitness of an individual. This is what we are going to use in this
paper, where we present a method that uses resampling via an
individual memory and use either explicit averaging or statistical
tests like the non-parametric Wilcoxon test. 
First we will examine and try to find the shape of the noise that
actually appears in games; then we will check in this paper what is the influence on the quality of
results of these two strategies and which one, if any, is the best
when working in noisy environments.  


\section{\uppercase{Conclusions}}
\label{sec:conclusion}

\noindent Please note that ONLY the files required to compile your paper should be submitted. Previous versions or examples MUST be removed from the compilation directory before submission.

We hope you find the information in this template useful in the preparation of your submission.

\section*{\uppercase{Acknowledgements}}

\noindent This work has been supported in part by SIPESCA (Programa Operativo
FEDER de Andalucía 2007-2013), TIN2014-56494-C4-3-P (Spanish
Ministry of Economy and Competitivity), SPIP2014-01437 (Direcci{\'o}n
General de Tr{\'a}fico), PRY142/14 (Fundaci{\'o}n P{\'u}blica Andaluza
Centro de Estudios Andaluces en la IX Convocatoria de Proyectos de
Investigaci{\'o}n), and PYR-2014-17 GENIL project (CEI-BIOTIC
Granada).

\vfill

\bibliographystyle{apalike}
\bibliography{geneura,GA-general,noisy}

\end{document}

